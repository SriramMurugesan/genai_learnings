{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Exercises for AI\n",
    "\n",
    "This notebook provides hands-on exercises with probability concepts essential for machine learning.\n",
    "\n",
    "## Learning Objectives\n",
    "- Work with probability distributions\n",
    "- Apply Bayes' theorem to real problems\n",
    "- Perform Monte Carlo simulations\n",
    "- Visualize probability concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Distributions\n",
    "\n",
    "### 1.1 Bernoulli Distribution\n",
    "Models binary outcomes (success/failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli distribution: Coin flip\n",
    "p = 0.6  # Probability of heads\n",
    "\n",
    "# Simulate 1000 coin flips\n",
    "flips = np.random.binomial(1, p, 1000)\n",
    "\n",
    "print(f\"Theoretical probability of heads: {p}\")\n",
    "print(f\"Empirical probability of heads: {flips.mean():.3f}\")\n",
    "print(f\"Number of heads: {flips.sum()}\")\n",
    "print(f\"Number of tails: {len(flips) - flips.sum()}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Tails (0)', 'Heads (1)'], [len(flips) - flips.sum(), flips.sum()], color=['red', 'blue'])\n",
    "plt.title('Bernoulli Distribution: Coin Flips (p=0.6)')\n",
    "plt.ylabel('Count')\n",
    "plt.axhline(y=500, color='gray', linestyle='--', label='Expected (p=0.5)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Binomial Distribution\n",
    "Number of successes in n independent trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binomial: Number of heads in 10 coin flips\n",
    "n_trials = 10\n",
    "p_success = 0.5\n",
    "n_experiments = 10000\n",
    "\n",
    "# Simulate\n",
    "results = np.random.binomial(n_trials, p_success, n_experiments)\n",
    "\n",
    "# Theoretical distribution\n",
    "x = np.arange(0, n_trials + 1)\n",
    "theoretical_pmf = stats.binom.pmf(x, n_trials, p_success)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(results, bins=np.arange(0, n_trials + 2) - 0.5, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.plot(x, theoretical_pmf, 'ro-', linewidth=2, label='Theoretical')\n",
    "plt.xlabel('Number of Heads')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Binomial Distribution (n={n_trials}, p={p_success})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cumulative = np.cumsum(theoretical_pmf)\n",
    "plt.plot(x, cumulative, 'go-', linewidth=2)\n",
    "plt.xlabel('Number of Heads')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean (theoretical): {n_trials * p_success}\")\n",
    "print(f\"Mean (empirical): {results.mean():.3f}\")\n",
    "print(f\"Variance (theoretical): {n_trials * p_success * (1 - p_success)}\")\n",
    "print(f\"Variance (empirical): {results.var():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normal (Gaussian) Distribution\n",
    "The most important distribution in statistics and ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal distribution parameters\n",
    "mu = 0  # mean\n",
    "sigma = 1  # standard deviation\n",
    "\n",
    "# Generate samples\n",
    "samples = np.random.normal(mu, sigma, 10000)\n",
    "\n",
    "# Theoretical distribution\n",
    "x = np.linspace(-4, 4, 100)\n",
    "pdf = stats.norm.pdf(x, mu, sigma)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.plot(x, pdf, 'r-', linewidth=2, label='Theoretical PDF')\n",
    "plt.axvline(mu, color='green', linestyle='--', linewidth=2, label=f'Mean = {mu}')\n",
    "plt.axvline(mu - sigma, color='orange', linestyle='--', label=f'Â±1Ïƒ')\n",
    "plt.axvline(mu + sigma, color='orange', linestyle='--')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title(f'Normal Distribution (Î¼={mu}, Ïƒ={sigma})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(samples, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (Normal Distribution Check)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 68-95-99.7 rule\n",
    "within_1_sigma = np.sum((samples >= mu - sigma) & (samples <= mu + sigma)) / len(samples)\n",
    "within_2_sigma = np.sum((samples >= mu - 2*sigma) & (samples <= mu + 2*sigma)) / len(samples)\n",
    "within_3_sigma = np.sum((samples >= mu - 3*sigma) & (samples <= mu + 3*sigma)) / len(samples)\n",
    "\n",
    "print(\"68-95-99.7 Rule Verification:\")\n",
    "print(f\"Within 1Ïƒ: {within_1_sigma:.1%} (expected: 68%)\")\n",
    "print(f\"Within 2Ïƒ: {within_2_sigma:.1%} (expected: 95%)\")\n",
    "print(f\"Within 3Ïƒ: {within_3_sigma:.1%} (expected: 99.7%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Comparing Multiple Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different normal distributions\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "distributions = [\n",
    "    {'mu': 0, 'sigma': 1, 'label': 'Î¼=0, Ïƒ=1', 'color': 'blue'},\n",
    "    {'mu': 0, 'sigma': 2, 'label': 'Î¼=0, Ïƒ=2', 'color': 'red'},\n",
    "    {'mu': 2, 'sigma': 1, 'label': 'Î¼=2, Ïƒ=1', 'color': 'green'},\n",
    "    {'mu': -2, 'sigma': 0.5, 'label': 'Î¼=-2, Ïƒ=0.5', 'color': 'orange'}\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for dist in distributions:\n",
    "    pdf = stats.norm.pdf(x, dist['mu'], dist['sigma'])\n",
    "    plt.plot(x, pdf, label=dist['label'], color=dist['color'], linewidth=2)\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Comparison of Normal Distributions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayes' Theorem\n",
    "\n",
    "### 2.1 Medical Test Example\n",
    "Classic example: Disease diagnosis with imperfect test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "P_disease = 0.01  # 1% of population has disease\n",
    "P_positive_given_disease = 0.99  # Test sensitivity (true positive rate)\n",
    "P_positive_given_no_disease = 0.05  # False positive rate\n",
    "\n",
    "# Calculate P(positive)\n",
    "P_no_disease = 1 - P_disease\n",
    "P_positive = (P_positive_given_disease * P_disease + \n",
    "              P_positive_given_no_disease * P_no_disease)\n",
    "\n",
    "# Bayes' theorem: P(disease | positive)\n",
    "P_disease_given_positive = (P_positive_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(\"Medical Test Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Disease prevalence: {P_disease:.1%}\")\n",
    "print(f\"Test sensitivity (true positive): {P_positive_given_disease:.1%}\")\n",
    "print(f\"False positive rate: {P_positive_given_no_disease:.1%}\")\n",
    "print(f\"\\nP(positive test): {P_positive:.2%}\")\n",
    "print(f\"\\nP(disease | positive test): {P_disease_given_positive:.2%}\")\n",
    "print(f\"\\nâš ï¸ Despite 99% accurate test, only {P_disease_given_positive:.1%} chance of having disease!\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Population breakdown\n",
    "population = 10000\n",
    "diseased = int(population * P_disease)\n",
    "healthy = population - diseased\n",
    "true_positive = int(diseased * P_positive_given_disease)\n",
    "false_positive = int(healthy * P_positive_given_no_disease)\n",
    "\n",
    "ax1.bar(['Diseased', 'Healthy'], [diseased, healthy], color=['red', 'green'])\n",
    "ax1.set_ylabel('Number of People')\n",
    "ax1.set_title(f'Population of {population:,} People')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test results\n",
    "categories = ['True\\nPositive', 'False\\nPositive', 'True\\nNegative', 'False\\nNegative']\n",
    "values = [true_positive, false_positive, healthy - false_positive, diseased - true_positive]\n",
    "colors = ['darkgreen', 'orange', 'lightgreen', 'darkred']\n",
    "\n",
    "ax2.bar(categories, values, color=colors)\n",
    "ax2.set_ylabel('Number of People')\n",
    "ax2.set_title('Test Results Breakdown')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(values):\n",
    "    ax2.text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOut of {true_positive + false_positive} positive tests:\")\n",
    "print(f\"  - True positives: {true_positive}\")\n",
    "print(f\"  - False positives: {false_positive}\")\n",
    "print(f\"  - Probability of disease: {true_positive/(true_positive + false_positive):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spam Filter Example\n",
    "Naive Bayes classifier for spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified spam filter\n",
    "P_spam = 0.3  # 30% of emails are spam\n",
    "\n",
    "# Word probabilities\n",
    "words = ['free', 'money', 'meeting', 'lunch']\n",
    "P_word_given_spam = [0.8, 0.7, 0.1, 0.05]\n",
    "P_word_given_ham = [0.1, 0.05, 0.6, 0.4]\n",
    "\n",
    "# Email contains: \"free money\"\n",
    "email_words = ['free', 'money']\n",
    "\n",
    "# Calculate P(spam | words)\n",
    "P_ham = 1 - P_spam\n",
    "\n",
    "# Naive assumption: words are independent\n",
    "P_words_given_spam = 1\n",
    "P_words_given_ham = 1\n",
    "\n",
    "for word in email_words:\n",
    "    idx = words.index(word)\n",
    "    P_words_given_spam *= P_word_given_spam[idx]\n",
    "    P_words_given_ham *= P_word_given_ham[idx]\n",
    "\n",
    "# Bayes' theorem\n",
    "numerator_spam = P_words_given_spam * P_spam\n",
    "numerator_ham = P_words_given_ham * P_ham\n",
    "denominator = numerator_spam + numerator_ham\n",
    "\n",
    "P_spam_given_words = numerator_spam / denominator\n",
    "P_ham_given_words = numerator_ham / denominator\n",
    "\n",
    "print(\"Spam Filter Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Email contains: {', '.join(email_words)}\")\n",
    "print(f\"\\nP(spam): {P_spam:.1%}\")\n",
    "print(f\"P(ham): {P_ham:.1%}\")\n",
    "print(f\"\\nP(words | spam): {P_words_given_spam:.3f}\")\n",
    "print(f\"P(words | ham): {P_words_given_ham:.3f}\")\n",
    "print(f\"\\nP(spam | words): {P_spam_given_words:.1%}\")\n",
    "print(f\"P(ham | words): {P_ham_given_words:.1%}\")\n",
    "print(f\"\\nðŸ“§ Classification: {'SPAM' if P_spam_given_words > 0.5 else 'HAM'}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(['Spam', 'Ham'], [P_spam_given_words, P_ham_given_words], color=['red', 'green'])\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Email Classification: \"{\" \".join(email_words)}\"')\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(y=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Simulations\n",
    "\n",
    "### 3.1 Estimating Ï€ using Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi(n_samples):\n",
    "    \"\"\"Estimate Ï€ by randomly sampling points in a square\"\"\"\n",
    "    # Generate random points in [0, 1] x [0, 1]\n",
    "    x = np.random.uniform(0, 1, n_samples)\n",
    "    y = np.random.uniform(0, 1, n_samples)\n",
    "    \n",
    "    # Check if points are inside quarter circle\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "    \n",
    "    # Ï€ â‰ˆ 4 * (points inside circle / total points)\n",
    "    pi_estimate = 4 * np.sum(inside_circle) / n_samples\n",
    "    \n",
    "    return pi_estimate, x, y, inside_circle\n",
    "\n",
    "# Run simulation\n",
    "n_samples = 10000\n",
    "pi_estimate, x, y, inside = estimate_pi(n_samples)\n",
    "\n",
    "print(f\"Estimated Ï€: {pi_estimate:.6f}\")\n",
    "print(f\"Actual Ï€: {np.pi:.6f}\")\n",
    "print(f\"Error: {abs(pi_estimate - np.pi):.6f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x[inside], y[inside], c='red', s=1, alpha=0.5, label='Inside circle')\n",
    "plt.scatter(x[~inside], y[~inside], c='blue', s=1, alpha=0.5, label='Outside circle')\n",
    "circle = plt.Circle((0, 0), 1, fill=False, color='black', linewidth=2)\n",
    "plt.gca().add_patch(circle)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title(f'Monte Carlo Estimation of Ï€\\n{n_samples:,} samples')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_sizes = np.logspace(2, 5, 50, dtype=int)\n",
    "estimates = [estimate_pi(n)[0] for n in sample_sizes]\n",
    "\n",
    "plt.semilogx(sample_sizes, estimates, 'b-', linewidth=2, label='Estimate')\n",
    "plt.axhline(y=np.pi, color='red', linestyle='--', linewidth=2, label='True Ï€')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Estimated Ï€')\n",
    "plt.title('Convergence to Ï€')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Central Limit Theorem Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Limit Theorem: Sample means approach normal distribution\n",
    "n_samples = 1000\n",
    "sample_sizes = [1, 5, 10, 30]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original distribution: Uniform [0, 1]\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    # Generate samples and compute means\n",
    "    sample_means = []\n",
    "    for _ in range(n_samples):\n",
    "        sample = np.random.uniform(0, 1, n)\n",
    "        sample_means.append(sample.mean())\n",
    "    \n",
    "    sample_means = np.array(sample_means)\n",
    "    \n",
    "    # Plot histogram\n",
    "    axes[idx].hist(sample_means, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Overlay normal distribution\n",
    "    mu = 0.5  # Mean of uniform [0, 1]\n",
    "    sigma = 1 / np.sqrt(12 * n)  # Standard error\n",
    "    x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "    pdf = stats.norm.pdf(x, mu, sigma)\n",
    "    axes[idx].plot(x, pdf, 'r-', linewidth=2, label='Normal PDF')\n",
    "    \n",
    "    axes[idx].set_xlabel('Sample Mean')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].set_title(f'Sample Size n = {n}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Central Limit Theorem: Distribution of Sample Means', fontsize=16, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Central Limit Theorem:\")\n",
    "print(\"As sample size increases, the distribution of sample means\")\n",
    "print(\"approaches a normal distribution, regardless of the original distribution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expected Value and Variance\n",
    "\n",
    "### 4.1 Calculating Expected Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice roll example\n",
    "outcomes = np.array([1, 2, 3, 4, 5, 6])\n",
    "probabilities = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "# Expected value: E[X] = Î£ x * P(x)\n",
    "expected_value = np.sum(outcomes * probabilities)\n",
    "\n",
    "# Variance: Var(X) = E[XÂ²] - (E[X])Â²\n",
    "expected_x_squared = np.sum(outcomes**2 * probabilities)\n",
    "variance = expected_x_squared - expected_value**2\n",
    "std_dev = np.sqrt(variance)\n",
    "\n",
    "print(\"Fair Dice Statistics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Expected value: {expected_value:.3f}\")\n",
    "print(f\"Variance: {variance:.3f}\")\n",
    "print(f\"Standard deviation: {std_dev:.3f}\")\n",
    "\n",
    "# Simulate and verify\n",
    "n_rolls = 100000\n",
    "rolls = np.random.choice(outcomes, size=n_rolls, p=probabilities)\n",
    "\n",
    "print(f\"\\nEmpirical (from {n_rolls:,} rolls):\")\n",
    "print(f\"Mean: {rolls.mean():.3f}\")\n",
    "print(f\"Variance: {rolls.var():.3f}\")\n",
    "print(f\"Std dev: {rolls.std():.3f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(outcomes, probabilities, color='steelblue', edgecolor='black')\n",
    "plt.axvline(expected_value, color='red', linestyle='--', linewidth=2, label=f'E[X] = {expected_value:.2f}')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Probability Mass Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rolls, bins=np.arange(0.5, 7.5, 1), density=True, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "plt.axvline(rolls.mean(), color='red', linestyle='--', linewidth=2, label=f'Sample Mean = {rolls.mean():.2f}')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title(f'Empirical Distribution ({n_rolls:,} rolls)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical ML Application: Probability in Classification\n",
    "\n",
    "### 5.1 Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"Convert logits to probabilities\"\"\"\n",
    "    exp_logits = np.exp(logits - np.max(logits))  # Numerical stability\n",
    "    return exp_logits / exp_logits.sum()\n",
    "\n",
    "# Example: Image classification logits\n",
    "logits = np.array([2.0, 1.0, 0.1])  # Raw scores from neural network\n",
    "class_names = ['Cat', 'Dog', 'Bird']\n",
    "\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Image Classification Example\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Logits (raw scores):\")\n",
    "for name, logit in zip(class_names, logits):\n",
    "    print(f\"  {name}: {logit:.2f}\")\n",
    "\n",
    "print(\"\\nProbabilities (after softmax):\")\n",
    "for name, prob in zip(class_names, probabilities):\n",
    "    print(f\"  {name}: {prob:.1%}\")\n",
    "\n",
    "print(f\"\\nSum of probabilities: {probabilities.sum():.6f}\")\n",
    "print(f\"Predicted class: {class_names[np.argmax(probabilities)]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.bar(class_names, logits, color='steelblue', edgecolor='black')\n",
    "ax1.set_ylabel('Logit Value')\n",
    "ax1.set_title('Raw Logits from Neural Network')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "colors = ['green' if p == probabilities.max() else 'lightblue' for p in probabilities]\n",
    "ax2.bar(class_names, probabilities, color=colors, edgecolor='black')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('Probabilities after Softmax')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (name, prob) in enumerate(zip(class_names, probabilities)):\n",
    "    ax2.text(i, prob + 0.02, f'{prob:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. **Probability Distributions**\n",
    "   - Bernoulli, Binomial, and Normal distributions\n",
    "   - Empirical vs theoretical distributions\n",
    "\n",
    "2. **Bayes' Theorem**\n",
    "   - Medical diagnosis example\n",
    "   - Spam filtering application\n",
    "\n",
    "3. **Monte Carlo Simulations**\n",
    "   - Estimating Ï€\n",
    "   - Central Limit Theorem\n",
    "\n",
    "4. **Expected Value and Variance**\n",
    "   - Theoretical calculations\n",
    "   - Empirical verification\n",
    "\n",
    "5. **ML Applications**\n",
    "   - Softmax function for classification\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Probability is fundamental to machine learning\n",
    "- Bayes' theorem enables probabilistic reasoning\n",
    "- Monte Carlo methods approximate complex distributions\n",
    "- Understanding distributions helps interpret ML model outputs\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to [Linear Algebra Operations](./02_linear_algebra_operations.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
