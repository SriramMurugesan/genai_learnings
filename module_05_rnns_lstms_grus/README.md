# Module 5: RNNs, LSTMs, and GRUs for Sequence Modeling

## Overview

This module covers recurrent neural networks for sequential data like text, time series, and audio. Learn RNNs, LSTMs, and GRUs for various sequence modeling tasks.

## Learning Objectives

- Understand sequence modeling
- Master RNN architecture
- Learn LSTM and GRU mechanisms
- Implement text generation
- Build time series forecasting models
- Create sentiment analysis systems

## Contents

### Theory Documents

1. **[RNN Fundamentals](./theory/01_rnn_fundamentals.md)**
 - Sequence modeling
 - Recurrent connections
 - Vanishing/exploding gradients
 - Backpropagation through time

2. **[LSTM Architecture](./theory/02_lstm_architecture.md)**
 - Long Short-Term Memory
 - Gates (forget, input, output)
 - Cell state
 - Solving vanishing gradients

3. **[GRU Architecture](./theory/03_gru_architecture.md)**
 - Gated Recurrent Unit
 - Simplified gating
 - Comparison with LSTM

### Practical Notebooks

1. **[RNN Implementation](./notebooks/01_rnn_implementation.ipynb)**
 - Build RNN from scratch
 - Character-level language model
 - Understanding recurrence

2. **[LSTM Text Generation](./notebooks/02_lstm_text_generation.ipynb)**
 - Character-level generation
 - Word-level generation
 - Shakespeare text generation

3. **[GRU Sequence Prediction](./notebooks/03_gru_sequence_prediction.ipynb)**
 - Sequence-to-sequence
 - Prediction tasks
 - Comparison with LSTM

4. **[Time Series Forecasting](./notebooks/04_time_series_forecasting.ipynb)**
 - Stock price prediction
 - Weather forecasting
 - Evaluation metrics

5. **[Sentiment Analysis](./notebooks/05_sentiment_analysis.ipynb)**
 - IMDB movie reviews
 - Text classification
 - Embeddings and LSTMs

## ‚è± Estimated Time

- Theory: 3-4 hours
- Notebooks: 5-6 hours
- Exercises: 2-3 hours
- **Total: 10-13 hours**

## Prerequisites

- Module 3: Neural Networks
- Understanding of sequences
- Text processing basics

## Course Completion

Congratulations on completing all modules!

### Next Steps:
1. Build your own projects
2. Participate in Kaggle competitions
3. Explore advanced topics (Transformers, GANs, etc.)
4. Contribute to open source
