{
 "cells": [
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
 "# Generative AI Demo with HuggingFace\\n",
 "\\n",
 "## Overview\\n",
 "This notebook demonstrates generative AI capabilities using HuggingFace Transformers:\\n",
 "- Text generation with GPT-2\\n",
 "- Image generation with Stable Diffusion\\n",
 "- Text-to-image with DALL-E mini\\n",
 "\\n",
 "**Note:** Some models require significant computational resources. Use Google Colab with GPU for best results."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
 "# Install required packages\\n",
 "!pip install -q transformers diffusers accelerate torch pillow\\n",
 "\\n",
 "import torch\\n",
 "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\\n",
 "from diffusers import StableDiffusionPipeline\\n",
 "from PIL import Image\\n",
 "import matplotlib.pyplot as plt\\n",
 "\\n",
 "# Check if GPU is available\\n",
 "device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n",
 "print(f'Using device: {device}')\\n",
 "if device == 'cuda':\\n",
 " print(f'GPU: {torch.cuda.get_device_name(0)}')\\n",
 "else:\\n",
 " print('\u26a0\ufe0f No GPU detected. Some models may be slow.')"
 ]
 },
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
 "## Part 1: Text Generation with GPT-2\\n",
 "\\n",
 "GPT-2 is an autoregressive language model that generates text by predicting the next word."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
 "# Load GPT-2 model\\n",
 "print('Loading GPT-2...')\\n",
 "text_generator = pipeline('text-generation', model='gpt2', device=0 if device=='cuda' else -1)\\n",
 "print('\u2705 Model loaded!')\\n",
 "\\n",
 "# Generate text\\n",
 "prompt = \\\"Artificial intelligence is\\\"\\n",
 "print(f'\\\\nPrompt: {prompt}')\\n",
 "print('\\\\nGenerating...')\\n",
 "\\n",
 "outputs = text_generator(\\n",
 " prompt,\\n",
 " max_length=100,\\n",
 " num_return_sequences=3,\\n",
 " temperature=0.8,\\n",
 " do_sample=True\\n",
 ")\\n",
 "\\n",
 "for i, output in enumerate(outputs, 1):\\n",
 " print(f'\\\\n--- Generation {i} ---')\\n",
 " print(output['generated_text'])"
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
 "# Experiment with different prompts\\n",
 "prompts = [\\n",
 " \\\"Once upon a time in a futuristic city,\\\",\\n",
 " \\\"The future of machine learning is\\\",\\n",
 " \\\"In the year 2050, robots\\\"\\n",
 "]\\n",
 "\\n",
 "for prompt in prompts:\\n",
 " print(f'\\\\n\ud83d\udcdd Prompt: {prompt}')\\n",
 " result = text_generator(prompt, max_length=60, num_return_sequences=1)[0]\\n",
 " print(f'Generated: {result[\\\"generated_text\\\"]}')"
 ]
 },
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
 "## Part 2: Sentiment Analysis (Discriminative Model for Comparison)\\n",
 "\\n",
 "Let's compare with a discriminative model that classifies text."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
 "# Load sentiment analysis model\\n",
 "sentiment_analyzer = pipeline('sentiment-analysis', device=0 if device=='cuda' else -1)\\n",
 "\\n",
 "texts = [\\n",
 " \\\"I love this product! It's amazing!\\\",\\n",
 " \\\"This is terrible. Very disappointed.\\\",\\n",
 " \\\"It's okay, nothing special.\\\"\\n",
 "]\\n",
 "\\n",
 "print('Sentiment Analysis Results:')\\n",
 "for text in texts:\\n",
 " result = sentiment_analyzer(text)[0]\\n",
 " print(f'\\\\nText: {text}')\\n",
 " print(f'Sentiment: {result[\\\"label\\\"]} (confidence: {result[\\\"score\\\"]:.2%})')"
 ]
 },
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
 "## Part 3: Image Generation with Stable Diffusion (Optional - Requires GPU)\\n",
 "\\n",
 "**Warning:** This requires significant GPU memory (8GB+). Skip if running on CPU."
 ]
 },
 {
 "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
 "# Only run if GPU is available\\n",
 "if device == 'cuda':\\n",
 " print('Loading Stable Diffusion...')\\n",
 " print('\u26a0\ufe0f This may take a few minutes...')\\n",
 " \\n",
 " # Load model (using smaller version for faster loading)\\n",
 " pipe = StableDiffusionPipeline.from_pretrained(\\n",
 " \\\"runwayml/stable-diffusion-v1-5\\\",\\n",
 " torch_dtype=torch.float16\\n",
 " )\\n",
 " pipe = pipe.to(device)\\n",
 " \\n",
 " print('\u2705 Model loaded!')\\n",
 " \\n",
 " # Generate image\\n",
 " prompt = \\\"A beautiful sunset over mountains, digital art\\\"\\n",
 " print(f'\\\\nPrompt: {prompt}')\\n",
 " print('Generating image...')\\n",
 " \\n",
 " image = pipe(prompt, num_inference_steps=30).images[0]\\n",
 " \\n",
 " # Display\\n",
 " plt.figure(figsize=(10, 10))\\n",
 " plt.imshow(image)\\n",
 " plt.axis('off')\\n",
 " plt.title(f'Generated: {prompt}')\\n",
 " plt.show()\\n",
 " \\n",
 " # Save\\n",
 " image.save('generated_image.png')\\n",
 " print('\u2705 Image saved as generated_image.png')\\n",
 "else:\\n",
 " print('\u26a0\ufe0f GPU not available. Skipping image generation.')\\n",
 " print('To run this section, use Google Colab with GPU runtime.')"
 ]
 },
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
 "## Part 4: Understanding Generative vs Discriminative\\n",
 "\\n",
 "### Key Differences\\n",
 "\\n",
 "**Discriminative Model (Sentiment Analysis):**\\n",
 "- Input: Text\\n",
 "- Output: Label (Positive/Negative)\\n",
 "- Learns: P(Y|X) - probability of label given input\\n",
 "\\n",
 "**Generative Model (GPT-2, Stable Diffusion):**\\n",
 "- Input: Prompt/Noise\\n",
 "- Output: New content (text/image)\\n",
 "- Learns: P(X) - probability distribution of data\\n",
 "\\n",
 "### Practical Implications\\n",
 "- Discriminative: Classification, prediction\\n",
 "- Generative: Content creation, data augmentation"
 ]
 },
 {
 "cell_type": "markdown",
 "metadata": {},
 "source": [
 "## \ud83c\udfaf Exercises\\n",
 "\\n",
 "1. **Text Generation:**\\n",
 " - Try different prompts\\n",
 " - Experiment with temperature (0.1 to 2.0)\\n",
 " - Compare outputs at different temperatures\\n",
 "\\n",
 "2. **Sentiment Analysis:**\\n",
 " - Analyze your own text samples\\n",
 " - Find edge cases where the model struggles\\n",
 "\\n",
 "3. **Image Generation (if GPU available):**\\n",
 " - Try different prompts\\n",
 " - Experiment with num_inference_steps\\n",
 " - Generate variations of the same prompt\\n",
 "\\n",
 "## Key Takeaways\\n",
 "- \u2705 HuggingFace makes generative AI accessible\\n",
 "- \u2705 Generative models create new content\\n",
 "- \u2705 Discriminative models classify existing content\\n",
 "- \u2705 Both types have important applications\\n",
 "\\n",
 "## Next Steps\\n",
 "- Explore more models on [HuggingFace Hub](https://huggingface.co/models)\\n",
 "- Learn about fine-tuning models\\n",
 "- Move to Module 2: Mathematical Foundations"
 ]
 }
 ],
 "metadata": {
 "kernelspec": {
 "display_name": "Python 3",
 "language": "python",
 "name": "python3"
 },
 "language_info": {
 "name": "python",
 "version": "3.10.0"
 }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}