{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "Build a complete neural network using only NumPy to understand the fundamentals.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement forward propagation\n",
    "- Implement backpropagation\n",
    "- Train a neural network on real data\n",
    "- Understand gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Activation Functions\n",
    "\n",
    "Implement common activation functions and their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh activation function\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Derivative of tanh\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Test activations\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
    "plt.plot(x, sigmoid_derivative(x), label='Sigmoid Derivative', linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Sigmoid')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(x, relu(x), label='ReLU')\n",
    "plt.plot(x, relu_derivative(x), label='ReLU Derivative', linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('ReLU')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(x, tanh(x), label='Tanh')\n",
    "plt.plot(x, tanh_derivative(x), label='Tanh Derivative', linestyle='--')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Tanh')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network Class\n",
    "\n",
    "Implement a flexible neural network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize neural network\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of layer sizes [input, hidden1, hidden2, ..., output]\n",
    "            activation: Activation function ('relu', 'sigmoid', 'tanh')\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # He initialization for ReLU, Xavier for others\n",
    "            if activation == 'relu':\n",
    "                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
    "            else:\n",
    "                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(1.0 / layer_sizes[i])\n",
    "            \n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def _activate(self, x):\n",
    "        \"\"\"Apply activation function\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return relu(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return sigmoid(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return tanh(x)\n",
    "    \n",
    "    def _activate_derivative(self, x):\n",
    "        \"\"\"Apply activation derivative\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return relu_derivative(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return sigmoid_derivative(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return tanh_derivative(x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            Output predictions\n",
    "        \"\"\"\n",
    "        self.z_values = []  # Pre-activation values\n",
    "        self.activations = [X]  # Post-activation values\n",
    "        \n",
    "        A = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            Z = A @ self.weights[i] + self.biases[i]\n",
    "            A = self._activate(Z)\n",
    "            \n",
    "            self.z_values.append(Z)\n",
    "            self.activations.append(A)\n",
    "        \n",
    "        # Output layer (sigmoid for binary classification)\n",
    "        Z = A @ self.weights[-1] + self.biases[-1]\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "        self.z_values.append(Z)\n",
    "        self.activations.append(A)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            y: True labels\n",
    "            learning_rate: Learning rate\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZ = self.activations[-1] - y\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # Compute gradients\n",
    "            dW = (1/m) * (self.activations[i].T @ dZ)\n",
    "            db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "            \n",
    "            if i > 0:\n",
    "                # Propagate to previous layer\n",
    "                dA = dZ @ self.weights[i].T\n",
    "                dZ = dA * self._activate_derivative(self.z_values[i-1])\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * dW\n",
    "            self.biases[i] -= learning_rate * db\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \n",
    "        Args:\n",
    "            X: Training data\n",
    "            y: Training labels\n",
    "            epochs: Number of training epochs\n",
    "            learning_rate: Learning rate\n",
    "            verbose: Print progress\n",
    "        \n",
    "        Returns:\n",
    "            List of losses\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Compute loss (binary cross-entropy)\n",
    "            loss = -np.mean(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                accuracy = np.mean((predictions > 0.5) == y)\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        predictions = self.forward(X)\n",
    "        return (predictions > 0.5).astype(int)\n",
    "\n",
    "print(\"Neural Network class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train on Moons Dataset\n",
    "\n",
    "Test the neural network on a non-linear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y.flatten()==0, 0], X[y.flatten()==0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X[y.flatten()==1, 0], X[y.flatten()==1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Moons Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train neural network\n",
    "nn = NeuralNetwork([2, 16, 8, 1], activation='relu')\n",
    "\n",
    "# Train\n",
    "losses = nn.train(X_train, y_train, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_predictions = nn.predict(X_test)\n",
    "test_accuracy = np.mean(test_predictions == y_test)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    plt.scatter(X[y.flatten()==0, 0], X[y.flatten()==0, 1], c='blue', label='Class 0', edgecolors='k')\n",
    "    plt.scatter(X[y.flatten()==1, 0], X[y.flatten()==1, 1], c='red', label='Class 1', edgecolors='k')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment with Different Architectures\n",
    "\n",
    "Try different network architectures and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different architectures\n",
    "architectures = [\n",
    "    [2, 8, 1],\n",
    "    [2, 16, 1],\n",
    "    [2, 16, 8, 1],\n",
    "    [2, 32, 16, 1]\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\nTraining architecture: {arch}\")\n",
    "    nn = NeuralNetwork(arch, activation='relu')\n",
    "    losses = nn.train(X_train, y_train, epochs=500, learning_rate=0.1, verbose=False)\n",
    "    \n",
    "    test_pred = nn.predict(X_test)\n",
    "    test_acc = np.mean(test_pred == y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'architecture': str(arch),\n",
    "        'accuracy': test_acc,\n",
    "        'final_loss': losses[-1]\n",
    "    })\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nResults Summary:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've implemented:\n",
    "- \u2705 Activation functions (sigmoid, ReLU, tanh)\n",
    "- \u2705 Forward propagation\n",
    "- \u2705 Backpropagation\n",
    "- \u2705 Complete training loop\n",
    "- \u2705 Decision boundary visualization\n",
    "- \u2705 Architecture comparison\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Neural networks can learn non-linear decision boundaries\n",
    "- Deeper networks can capture more complex patterns\n",
    "- Proper initialization is important\n",
    "- ReLU works well for hidden layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}