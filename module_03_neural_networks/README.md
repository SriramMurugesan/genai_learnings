# Module 3: Neural Networks & Backpropagation

## Overview

This module covers the fundamentals of neural networks, the building blocks of deep learning. You'll learn how neural networks work, implement them from scratch, and use modern frameworks.

## Learning Objectives

- Understand neural network architecture
- Implement forward propagation
- Master backpropagation algorithm
- Build neural networks from scratch (NumPy)
- Use PyTorch and TensorFlow/Keras
- Train models on real datasets

## Contents

### Theory Documents

1. **[Neural Network Fundamentals](./theory/01_neural_network_fundamentals.md)**
 - Perceptrons and neurons
 - Multi-layer networks
 - Activation functions
 - Universal approximation theorem

2. **[Backpropagation](./theory/02_backpropagation.md)**
 - Gradient descent
 - Chain rule
 - Backpropagation algorithm
 - Training process

3. **[Activation Functions](./theory/03_activation_functions.md)**
 - Sigmoid, Tanh, ReLU
 - Leaky ReLU, ELU, GELU
 - When to use which

### Practical Notebooks

1. **[Neural Network from Scratch](./notebooks/01_nn_from_scratch.ipynb)**
 - Implement using only NumPy
 - Forward and backward propagation
 - Training on XOR problem

2. **[PyTorch Basics](./notebooks/02_pytorch_basics.ipynb)**
 - Tensors and autograd
 - Building models
 - Training loop

3. **[TensorFlow/Keras Basics](./notebooks/03_tensorflow_keras_basics.ipynb)**
 - Sequential and Functional API
 - Model compilation
 - Training and evaluation

4. **[Training MNIST](./notebooks/04_training_mnist.ipynb)**
 - Complete classification pipeline
 - Handwritten digit recognition
 - Model evaluation and visualization

## ‚è± Estimated Time

- Theory: 3-4 hours
- Notebooks: 4-5 hours
- Exercises: 2-3 hours
- **Total: 9-12 hours**

## Prerequisites

- Module 1: AI/ML/DL Overview
- Module 2: Mathematical Foundations
- Basic Python and NumPy

## Next Module

[Module 4: CNNs for Feature Extraction](../module_04_cnns/)
